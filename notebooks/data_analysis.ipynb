{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T13:11:27.601249Z",
     "start_time": "2024-11-28T13:11:25.767689Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296495f905c70416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T17:46:41.269089Z",
     "start_time": "2024-11-26T17:46:41.207384Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert numpy.ndarray to numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/train.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index_col\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mID\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\u001B[38;5;241m.\u001B[39mread(nrows)\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1968\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1965\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1966\u001B[0m         new_col_dict \u001B[38;5;241m=\u001B[39m col_dict\n\u001B[0;32m-> 1968\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(\n\u001B[1;32m   1969\u001B[0m         new_col_dict,\n\u001B[1;32m   1970\u001B[0m         columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[1;32m   1971\u001B[0m         index\u001B[38;5;241m=\u001B[39mindex,\n\u001B[1;32m   1972\u001B[0m         copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m using_copy_on_write(),\n\u001B[1;32m   1973\u001B[0m     )\n\u001B[1;32m   1975\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_currow \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m new_rows\n\u001B[1;32m   1976\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/frame.py:778\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    772\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[1;32m    773\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[1;32m    774\u001B[0m     )\n\u001B[1;32m    776\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[0;32m--> 778\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m dict_to_mgr(data, index, columns, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy, typ\u001B[38;5;241m=\u001B[39mmanager)\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/internals/construction.py:443\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[0;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    441\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mseries\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Series\n\u001B[0;32m--> 443\u001B[0m     arrays \u001B[38;5;241m=\u001B[39m Series(data, index\u001B[38;5;241m=\u001B[39mcolumns, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mobject\u001B[39m)\n\u001B[1;32m    444\u001B[0m     missing \u001B[38;5;241m=\u001B[39m arrays\u001B[38;5;241m.\u001B[39misna()\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;66;03m# GH10856\u001B[39;00m\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;66;03m# raise ValueError if only scalars in dict\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/series.py:490\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    487\u001B[0m name \u001B[38;5;241m=\u001B[39m ibase\u001B[38;5;241m.\u001B[39mmaybe_extract_name(name, data, \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m))\n\u001B[1;32m    489\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 490\u001B[0m     index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    493\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_dtype(dtype)\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/indexes/base.py:7647\u001B[0m, in \u001B[0;36mensure_index\u001B[0;34m(index_like, copy)\u001B[0m\n\u001B[1;32m   7645\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m MultiIndex\u001B[38;5;241m.\u001B[39mfrom_arrays(index_like)\n\u001B[1;32m   7646\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 7647\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m Index(index_like, copy\u001B[38;5;241m=\u001B[39mcopy, tupleize_cols\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   7648\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   7649\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Index(index_like, copy\u001B[38;5;241m=\u001B[39mcopy)\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/indexes/base.py:565\u001B[0m, in \u001B[0;36mIndex.__new__\u001B[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001B[0m\n\u001B[1;32m    562\u001B[0m         data \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39masarray_tuplesafe(data, dtype\u001B[38;5;241m=\u001B[39m_dtype_obj)\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 565\u001B[0m     arr \u001B[38;5;241m=\u001B[39m sanitize_array(data, \u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy)\n\u001B[1;32m    566\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    567\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex must be specified when data is not list-like\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(err):\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/construction.py:654\u001B[0m, in \u001B[0;36msanitize_array\u001B[0;34m(data, index, dtype, copy, allow_2d)\u001B[0m\n\u001B[1;32m    651\u001B[0m     subarr \u001B[38;5;241m=\u001B[39m _try_cast(data, dtype, copy)\n\u001B[1;32m    653\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 654\u001B[0m     subarr \u001B[38;5;241m=\u001B[39m maybe_convert_platform(data)\n\u001B[1;32m    655\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m subarr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mobject\u001B[39m:\n\u001B[1;32m    656\u001B[0m         subarr \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mndarray, subarr)\n",
      "File \u001B[0;32m~/anaconda3/envs/energy-forecasting/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:138\u001B[0m, in \u001B[0;36mmaybe_convert_platform\u001B[0;34m(values)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m _dtype_obj:\n\u001B[1;32m    137\u001B[0m     arr \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mndarray, arr)\n\u001B[0;32m--> 138\u001B[0m     arr \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mmaybe_convert_objects(arr)\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr\n",
      "File \u001B[0;32mlib.pyx:2538\u001B[0m, in \u001B[0;36mpandas._libs.lib.maybe_convert_objects\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Cannot convert numpy.ndarray to numpy.ndarray"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv', index_col=\"ID\")\n",
    "test = pd.read_csv('../data/test.csv', index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb297facb221464b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T10:10:45.471309Z",
     "start_time": "2024-11-25T10:10:45.453600Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32878fc3f9ca5361",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T18:14:55.883050Z",
     "start_time": "2024-11-26T18:14:54.849040Z"
    }
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee2c0c79f7f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f8730476e3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf2590-ba7e-4e58-83d8-e06869358648",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a683f1c335fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(f\"Number of missing values in train: {train.isnull().sum().sum()}\")\n",
    "# print only the columns with missing values and the number of missing values per column in train, if no missing values, no output\n",
    "print(train.isnull().sum()[train.isnull().sum() > 0])\n",
    "# Percentage of missing values in clouds\n",
    "print(f\"Percentage of missing values in clouds: {train['clouds'].isnull().sum() / len(train['clouds']) * 100}\")\n",
    "print(f\"Number of missing values in test: {test.isnull().sum().sum()}\")\n",
    "print(test.isnull().sum()[test.isnull().sum() > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786888126e0dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every missing value, we will interpolate the result from the previous and next value\n",
    "train = train.interpolate()\n",
    "print(f\"Number of missing values in train: {train.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165d3b7-4e14-4753-88fe-df78e17ac436",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['measurement_time'] = pd.to_datetime(train['measurement_time'])\n",
    "# plot all values of target across measurement_time\n",
    "plt.plot(train['measurement_time'], train['target'])\n",
    "plt.xlabel('Measurement Time')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Target vs Measurement Time')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ac207-df0b-4458-9142-3d7ec74cc140",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T14:43:37.687559Z",
     "start_time": "2024-11-28T14:43:37.676840Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'fall'\n",
    "    else:\n",
    "        return 'winter'\n",
    "def label_time_of_day(hour):\n",
    "    if hour in [6,7,8,9,10,11,12]:\n",
    "        return 'morning'\n",
    "    elif hour in [13,14,15,16,17]:\n",
    "        return 'afternoon'\n",
    "    elif hour in [18,19,20,21,22]:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2141a8-7e5d-42ae-ba70-7fe6a1322109",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def add_lagged_features(column: pd.Series, lags=7) -> pd.Series:\n",
    "    lag = []  # Initialize an empty list to store lagged values\n",
    "    for i in range(len(column)):\n",
    "        # Create a lag for each value in the column\n",
    "        if i + lags < len(column):\n",
    "            lag.append(column[i + lags])  # Append the value shifted by 'lags'\n",
    "        else:\n",
    "            lag.append(None)  # Append None (or np.nan) for out-of-bound indices\n",
    "    \n",
    "    # Convert the list of lagged values to a pandas Series and return it\n",
    "    lag = pd.Series(lag, index=column.index)\n",
    "    lag = lag.interpolate()\n",
    "    return lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14292e0-3c9c-4c54-8409-fcd7ce838e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate time_measurement into hour, day of the week and month column\n",
    "train_separate = train.copy()\n",
    "train_separate['hour'] = train_separate['measurement_time'].dt.hour\n",
    "train_separate['day_of_week'] = train_separate['measurement_time'].dt.dayofweek\n",
    "train_separate['month'] = train_separate['measurement_time'].dt.month\n",
    "#train_separate['year'] = train_separate['measurement_time'].dt.year\n",
    "train_separate = train_separate.drop(columns=['measurement_time'])\n",
    "# Apply season labeling based on the 'Month' column\n",
    "train_separate['season'] = train_separate['month'].apply(label_season).copy()\n",
    "# One-hot encode the 'Season' column\n",
    "train_separate = pd.get_dummies(train_separate, columns=['season'], drop_first=False).copy()\n",
    "train_separate = train_separate.drop(columns=['month'])\n",
    "train_separate['time_of_day'] = train_separate['hour'].apply(label_time_of_day).copy()\n",
    "# One-hot encode the 'Season' column\n",
    "train_separate = pd.get_dummies(train_separate, columns=['time_of_day'], drop_first=False).copy()\n",
    "train_separate = train_separate.drop(columns=['hour'])\n",
    "for column_label in ['mean_room_temperature', 'outside_temperature', 'wind_speed', 'clouds']:\n",
    "    train_separate[column_label + '_lag'] = add_lagged_features(train_separate[column_label])\n",
    "# print the variance of each column\n",
    "#print(train_separate.describe())\n",
    "print(train_separate.columns)\n",
    "print(f\"Number of missing values in train_separate: {train_separate.isnull().sum().sum()}\")\n",
    "# print only the columns with missing values and the number of missing values per column in train, if no missing values, no output\n",
    "print(train_separate.isnull().sum()[train_separate.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3fe8d74a2e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of correlation between numerical columns\n",
    "# only numerical columns are considered not measurement_time\n",
    "train_numerical = train_separate.copy()\n",
    "matrix = train_numerical.corr()\n",
    "# print highest correlations var 1 var 2 corr\n",
    "print(matrix.unstack().sort_values().drop_duplicates().tail(10))\n",
    "\n",
    "plt.matshow(matrix)\n",
    "# include the column names\n",
    "plt.xticks(range(train_numerical.shape[1]), train_numerical.columns, fontsize=14, rotation=90)\n",
    "plt.yticks(range(train_numerical.shape[1]), train_numerical.columns, fontsize=14)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9101c755b8cf22f",
   "metadata": {},
   "source": [
    "According to  Pearson correlation coefficient, $r > 0.75$ is considered as highly correlated. This mainly affects our results with radiation values. The following pairs of variables are highly correlated:\n",
    "```\n",
    "sun_radiation_north          sun_radiation_perpendicular    0.847473\n",
    "sun_radiation_perpendicular  sun_radiation_south            0.861518\n",
    "```\n",
    "Therefore we drop `sun_radiation_perpendicular` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83d978-08f4-4233-999f-95b549583162",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_numerical.copy()\n",
    "X = X.drop(columns=['target'])\n",
    "y = train_numerical['target']\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b30ccd-ce54-427a-8ff6-b5ddb76d4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train: {len(X_train)}, X_val: {len(X_val)}, Y_train: {len(y_train)}, Y_val: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df081c493c4809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Drop 'sun_radiation_perpendicular' for Version 2\n",
    "X2_train = X_train.drop(columns=['sun_radiation_perpendicular'])\n",
    "X2_val = X_val.drop(columns=['sun_radiation_perpendicular'])\n",
    "\n",
    "# Standardize the features using consistent fit and transform\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data and transform both training and validation data\n",
    "X1_scaled_train = scaler1.fit_transform(X_train)\n",
    "X1_scaled_val = scaler1.transform(X_val)\n",
    "\n",
    "X2_scaled_train = scaler2.fit_transform(X2_train)\n",
    "X2_scaled_val = scaler2.transform(X2_val)\n",
    "\n",
    "# Apply PCA\n",
    "pca1 = PCA()\n",
    "pca2 = PCA()\n",
    "\n",
    "# Fit PCA on training data and transform both training and validation data\n",
    "X1_pca_train = pca1.fit_transform(X1_scaled_train)\n",
    "X1_pca_val = pca1.transform(X1_scaled_val)\n",
    "\n",
    "X2_pca_train = pca2.fit_transform(X2_scaled_train)\n",
    "X2_pca_val = pca2.transform(X2_scaled_val)\n",
    "\n",
    "# Train linear regression models\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X1_pca_train, y_train)\n",
    "\n",
    "model2 = LinearRegression()\n",
    "model2.fit(X2_pca_train, y_train)\n",
    "\n",
    "model3 = LinearRegression()\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y1_pred = model1.predict(X1_pca_val)\n",
    "y2_pred = model2.predict(X2_pca_val)\n",
    "y3_pred = model3.predict(X_val)\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE1 = sqrt(mean_squared_error(y_val, y1_pred)) \n",
    "RMSE2 = sqrt(mean_squared_error(y_val, y2_pred)) \n",
    "RMSE3 = sqrt(mean_squared_error(y_val, y3_pred)) \n",
    "\n",
    "print(f\"RMSE for PCA when keeping all components: {RMSE1}\")\n",
    "print(f\"RMSE for PCA without sun_radiation_perpendicular: {RMSE2}\")\n",
    "print(f\"RMSE for basic: {RMSE3}\")\n",
    "\n",
    "# Check explained variance ratio\n",
    "print(f\"Explained variance ratio for all components (X1_pca): {pca1.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"Explained variance ratio without sun_radiation_perpendicular (X2_pca): {pca2.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da22e71c-f0db-49ed-bb54-8b1086d07170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffe0a0e0480ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = X.copy()\n",
    "# apply log transformation to all numerical columns except temperature\n",
    "train_log['sun_radiation_north'] = np.log1p(train_log['sun_radiation_north'])\n",
    "train_log['sun_radiation_perpendicular'] = np.log1p(train_log['sun_radiation_perpendicular'])\n",
    "train_log['sun_radiation_south'] = np.log1p(train_log['sun_radiation_south'])\n",
    "train_log['sun_radiation_east'] = np.log1p(train_log['sun_radiation_east'])\n",
    "train_log['sun_radiation_west'] = np.log1p(train_log['sun_radiation_west'])\n",
    "#train_log['wind_speed'] = np.log1p(train_log['wind_speed'])\n",
    "#train_log['wind_direction'] = np.log1p(train_log['wind_direction'])\n",
    "#train_log['source_1_temperature'] = np.log1p(train_log['source_1_temperature'])\n",
    "#train_log['source_2_temperature'] = np.log1p(train_log['source_2_temperature'])\n",
    "#train_log['source_3_temperature'] = np.log1p(train_log['source_3_temperature'])\n",
    "#train_log['source_4_temperature'] = np.log1p(train_log['source_4_temperature'])\n",
    "#train_log['mean_room_temperature'] = np.log1p(train_log['mean_room_temperature'])\n",
    "train_log['outside_temperature'] = np.log1p(train_log['outside_temperature'])\n",
    "train_log['clouds'] = np.log1p(train_log['clouds'])\n",
    "\n",
    "print(f\"Number of missing values in train: {train_log.isnull().sum().sum()}\")\n",
    "# print only the columns with missing values and the number of missing values per column in train, if no missing values, no output\n",
    "print(train_log.isnull().sum()[train_log.isnull().sum() > 0])\n",
    "train_log = train_log.interpolate()\n",
    "print(train_log.isnull().sum()[train_log.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc50d9887ab6179",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-28T15:15:08.139570Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# normalise the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_normalised = X.copy()\n",
    "train_normalised[train_normalised.columns] = scaler.fit_transform(train_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302dd31-334b-47de-8d1f-e45c82cabda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the data\n",
    "scaler = StandardScaler()\n",
    "train_normalised_std = X.copy()\n",
    "train_normalised_std[train_normalised_std.columns] = scaler.fit_transform(train_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be1f7336aaa238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T11:58:44.420058Z",
     "start_time": "2024-11-27T11:58:44.393280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit a linear regression model on the log and normalised data\n",
    "X1 = train_log\n",
    "X2 = train_normalised\n",
    "X3 = train_normalised_std\n",
    "X4 = train_numerical.drop(columns=['target'])\n",
    "\n",
    "X = [X1, X2, X3, X4]\n",
    "labels = [\" logged\", \" normalised minmax\", \" normalised standard\", \"\"]\n",
    "\n",
    "for i in range(len(X)):\n",
    "    data = X[i]\n",
    "    datay = y\n",
    "    label : str = labels[i]\n",
    "    train_size = int(len(data) * 0.8)\n",
    "    X_train, X_test = data[:train_size], data[train_size:]\n",
    "    y_train, y_test = datay[:train_size], datay[train_size:]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict the target variable on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate R-squared to see how well the target is predicted by the PCA components\n",
    "    rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "    # Output the amount of variance explained by the PCA components in the target\n",
    "    print(\"RMSE with\" + label + f\" numerical components: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1223a78-b18d-40d9-81e5-239a486e052b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T13:59:17.207914Z",
     "start_time": "2024-11-28T13:59:17.205291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do minmax and log\n",
    "N = int(len(train_log) * 0.8)\n",
    "X_train, X_test = train_log[:N], train_log[N:]\n",
    "y_train, y_test = y[:N], y[N:]\n",
    "\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "X_train[X_train.columns] = scaler1.fit_transform(X_train)\n",
    "X_test[X_test.columns] = scaler2.fit_transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#calculate RMSE\n",
    "RMSE1 = sqrt(mean_squared_error(y_test, y_pred)) \n",
    "print(f\"RMSE for minmax and log: {RMSE1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e975f0-c6ba-4516-b293-171090ff2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do minmax and log with PCA\n",
    "N = int(len(train_log) * 0.8)\n",
    "X_train, X_test = train_log[:N], train_log[N:]\n",
    "y_train, y_test = y[:N], y[N:]\n",
    "\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "X_train[X_train.columns] = scaler1.fit_transform(X_train)\n",
    "X_test[X_test.columns] = scaler2.fit_transform(X_test)\n",
    "pca = PCA()\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_test = pca.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_pca_train, y_train)\n",
    "y_pred = model.predict(X_pca_test)\n",
    "\n",
    "#calculate RMSE\n",
    "RMSE1 = sqrt(mean_squared_error(y_test, y_pred)) \n",
    "print(f\"RMSE for PCA with minmax and log: {RMSE1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (energy-forecasting)",
   "language": "python",
   "name": "energy-forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
